{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 4-28 to Code 4-51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing 2 datasets - one for creating the features (df_orig) and another one for identifying the \"named entity\" tags(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "t1 = pd.read_csv(\"ner.csv\",encoding='latin1', error_bad_lines=False)\n",
    "df_orig = pd.read_csv(\"ner_dataset.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O\n",
       "20          NaN           from   IN      O\n",
       "21          NaN           that   DT      O\n",
       "22          NaN        country   NN      O\n",
       "23          NaN              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O\n",
       "25          NaN             of   IN      O\n",
       "26          NaN       soldiers  NNS      O\n",
       "27          NaN         killed  VBN      O\n",
       "28          NaN             in   IN      O\n",
       "29          NaN            the   DT      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>...</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>london</td>\n",
       "      <td>to</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>...</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>to</td>\n",
       "      <td>protest</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>VB</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>...</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>protest</td>\n",
       "      <td>the</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>war</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the</td>\n",
       "      <td>war</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>in</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>war</td>\n",
       "      <td>...</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n",
       "0           0  thousand         of        demonstr           NNS   \n",
       "1           1        of   demonstr            have           VBP   \n",
       "2           2  demonstr       have           march           VBN   \n",
       "3           3      have      march         through            IN   \n",
       "4           4     march    through          london           NNP   \n",
       "5           5   through     london              to            TO   \n",
       "6           6    london         to         protest            VB   \n",
       "7           7        to    protest             the            DT   \n",
       "8           8   protest        the             war            NN   \n",
       "9           9       the        war              in            IN   \n",
       "\n",
       "  next-next-shape next-next-word next-pos   next-shape      next-word  ...  \\\n",
       "0       lowercase  demonstrators       IN    lowercase             of  ...   \n",
       "1       lowercase           have      NNS    lowercase  demonstrators  ...   \n",
       "2       lowercase        marched      VBP    lowercase           have  ...   \n",
       "3       lowercase        through      VBN    lowercase        marched  ...   \n",
       "4     capitalized         London       IN    lowercase        through  ...   \n",
       "5       lowercase             to      NNP  capitalized         London  ...   \n",
       "6       lowercase        protest       TO    lowercase             to  ...   \n",
       "7       lowercase            the       VB    lowercase        protest  ...   \n",
       "8       lowercase            war       DT    lowercase            the  ...   \n",
       "9       lowercase             in       NN    lowercase            war  ...   \n",
       "\n",
       "  prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word   prev-shape  \\\n",
       "0      __start2__    __START2__        wildcard     __START2__     wildcard   \n",
       "1      __start1__    __START1__        wildcard     __START1__  capitalized   \n",
       "2        thousand           NNS     capitalized      Thousands    lowercase   \n",
       "3              of            IN       lowercase             of    lowercase   \n",
       "4        demonstr           NNS       lowercase  demonstrators    lowercase   \n",
       "5            have           VBP       lowercase           have    lowercase   \n",
       "6           march           VBN       lowercase        marched    lowercase   \n",
       "7         through            IN       lowercase        through  capitalized   \n",
       "8          london           NNP     capitalized         London    lowercase   \n",
       "9              to            TO       lowercase             to    lowercase   \n",
       "\n",
       "       prev-word sentence_idx        shape           word    tag  \n",
       "0     __START1__          1.0  capitalized      Thousands      O  \n",
       "1      Thousands          1.0    lowercase             of      O  \n",
       "2             of          1.0    lowercase  demonstrators      O  \n",
       "3  demonstrators          1.0    lowercase           have      O  \n",
       "4           have          1.0    lowercase        marched      O  \n",
       "5        marched          1.0    lowercase        through      O  \n",
       "6        through          1.0  capitalized         London  B-geo  \n",
       "7         London          1.0    lowercase             to      O  \n",
       "8             to          1.0    lowercase        protest      O  \n",
       "9        protest          1.0    lowercase            the      O  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.columns = [\"sentence_id\",\"word\",\"pos\",\"tag\"]\n",
    "df_orig[\"sentence_id\"] = df_orig[\"sentence_id\"].fillna('none')\n",
    "df_orig[\"tag\"] = df_orig[\"tag\"].fillna('none') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below output - B and I represent the beginning or intermediate. We are interested in the ‘org’ tag wherever they are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code reconstructs the sentence in df_orig and gets the POS tags of the word level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "list_sent = []\n",
    "word_list = []\n",
    "tag_list = []\n",
    "for ind,row in df_orig[0:100000].iterrows():\n",
    "    sid = row[\"sentence_id\"]\n",
    "    word = row[\"word\"]\n",
    "    tag = row[\"tag\"]\n",
    "\n",
    "    if((sid!=\"none\") or (ind==0)):\n",
    "        if(len(word_list)>0):\n",
    "            list_sent.append(word_list)\n",
    "            pos_tags_list = nltk.pos_tag(word_list)\n",
    "            df = pd.DataFrame(pos_tags_list)\n",
    "            df[\"id\"] = sid_perm\n",
    "            \n",
    "            try:\n",
    "                df[\"tag\"] = tag_list\n",
    "            except:\n",
    "                print (tag_list,word_list,len(tag_list),len(word_list)) \n",
    "            if(sid_perm==\"Sentence: 1\"):\n",
    "                df_all_pos = df\n",
    "                \n",
    "            else:\n",
    "                df_all_pos = pd.concat([df_all_pos,df],axis=0)\n",
    "            \n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        word_list.append(word)\n",
    "        tag_list.append(tag)\n",
    "    else:\n",
    "         word_list.append(word)\n",
    "        #word_list = word_list + word + \" \"\n",
    "         tag_list.append(tag)\n",
    "        \n",
    "    if(sid!=\"none\"):\n",
    "        sid_perm = sid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99998, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0    1           id tag\n",
       "0      Thousands  NNS  Sentence: 1   O\n",
       "1             of   IN  Sentence: 1   O\n",
       "2  demonstrators  NNS  Sentence: 1   O\n",
       "3           have  VBP  Sentence: 1   O\n",
       "4        marched  VBN  Sentence: 1   O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###think space is creating issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pos.columns = [\"word\",\"pos_tag\",\"sid\",\"tag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_all is the data frame that contains the word and “POS” tags. Once this step is done we could get lemma of the word, shape of the word, length etc which are not dependent on the sentence but just the words themselves\n",
    "\n",
    "We would also want to identify words that have either numbers or special characters in them. We would tag these words into a seperate word type before replacing the special characters for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pos[\"word1\"] = df_all_pos[\"word\"] \n",
    "\n",
    "df_all_pos[\"word1\"] = df_all_pos.word1.str.replace('[^a-z\\s]+','') \n",
    "\n",
    "df_all_pos[\"word_type\"] = \"normal\"\n",
    "df_all_pos.loc[df_all_pos.word.str.contains('[0-9]+'),\"word_type\"] = \"number\"\n",
    "df_all_pos.loc[df_all_pos.word.str.contains('[^a-zA-Z\\s]+'),\"word_type\"] = \"special_chars\"\n",
    "\n",
    "df_all_pos.loc[(df_all_pos.word_type!=\"normal\") & (df_all_pos.word1.str.len()==0),\"word_type\"] = \"only_special\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_func(x):\n",
    "    return lemmatizer.lemmatize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pos[\"shape\"] = \"mixed\"\n",
    "df_all_pos.loc[((df_all_pos.word.str.islower()==True) & (df_all_pos.word_type==\"normal\")),\"shape\"]=\"lower\"\n",
    "df_all_pos.loc[((df_all_pos.word.str.islower()==False) & (df_all_pos.word_type==\"normal\")),\"shape\"]=\"upper\"\n",
    "\n",
    "df_all_pos[\"lemma\"] = df_all_pos[\"word\"].apply(lemma_func)\n",
    "\n",
    "df_all_pos[\"length\"] = df_all_pos[\"word\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the “relative position” of the word in the sentences and the sentence length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pos[\"ind_num\"] = df_all_pos.index\n",
    "df_all_pos[\"sent_len\"]=df_all_pos.groupby([\"sid\"])[\"ind_num\"].transform(max)\n",
    "df_all_pos1 = df_all_pos[df_all_pos.sent_len>0]\n",
    "df_all_pos1[\"rel_position\"] = df_all_pos1[\"ind_num\"] /df_all_pos1[\"sent_len\"]*100\n",
    "df_all_pos1[\"rel_position\"] = df_all_pos1[\"rel_position\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'pos_tag', 'sid', 'tag', 'word1', 'word_type', 'shape', 'lemma',\n",
       "       'length', 'ind_num', 'sent_len', 'rel_position'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_pos1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the pandas “shift” functionality to get features like previous word, next word, POS tag of the previous word, POS tag of the next tag etc. For each feature we call the get_prev_next to get the shifted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_next(df_all_pos,col_imp):\n",
    "    prev_col = \"prev_\" + col_imp\n",
    "    next_col = col_imp + \"_next\"\n",
    "    \n",
    "    prev_col1 = \"prev_prev_\" + col_imp\n",
    "    next_col1 = col_imp + \"_next_next\"\n",
    "    \n",
    "    df_all_pos[prev_col] = df_all_pos[col_imp].shift(1)\n",
    "    df_all_pos.loc[df_all_pos.index==0,prev_col] = \"start\"\n",
    "    \n",
    "    df_all_pos[next_col] = df_all_pos[col_imp].shift(-1)\n",
    "    df_all_pos.loc[df_all_pos.index==df_all_pos.sent_len,next_col] = \"end\"\n",
    "    \n",
    "    df_all_pos[prev_col1] = df_all_pos[col_imp].shift(2)\n",
    "    df_all_pos.loc[df_all_pos.index<2,prev_col1] = \"start\"\n",
    "    \n",
    "    df_all_pos[next_col1] = df_all_pos[col_imp].shift(-2)\n",
    "    df_all_pos.loc[(df_all_pos.sent_len-df_all_pos.index)<=1,next_col1] = \"end\"\n",
    "    \n",
    "    return df_all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pos1 = get_prev_next(df_all_pos1,\"word\")\n",
    "df_all_pos1 = get_prev_next(df_all_pos1,\"lemma\")\n",
    "df_all_pos1 = get_prev_next(df_all_pos1,\"shape\")\n",
    "df_all_pos1 = get_prev_next(df_all_pos1,\"pos_tag\")\n",
    "df_all_pos1 = get_prev_next(df_all_pos1,\"word_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'pos_tag', 'sid', 'tag', 'word1', 'word_type', 'shape', 'lemma',\n",
       "       'length', 'ind_num', 'sent_len', 'rel_position', 'prev_word',\n",
       "       'word_next', 'prev_prev_word', 'word_next_next', 'prev_lemma',\n",
       "       'lemma_next', 'prev_prev_lemma', 'lemma_next_next', 'prev_shape',\n",
       "       'shape_next', 'prev_prev_shape', 'shape_next_next', 'prev_pos_tag',\n",
       "       'pos_tag_next', 'prev_prev_pos_tag', 'pos_tag_next_next',\n",
       "       'prev_word_type', 'word_type_next', 'prev_prev_word_type',\n",
       "       'word_type_next_next'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_pos1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a binary \"org/no_org\" for the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = df_all_pos1[df_all_pos1.tag.isna()==False]\n",
    "t2[\"tag1\"] = \"no_org\"\n",
    "t2.loc[t2.tag.str.contains('org',case=False),\"tag1\"]=\"org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        84723\n",
       "B-geo     3303\n",
       "B-org     1876\n",
       "I-per     1846\n",
       "B-tim     1823\n",
       "B-gpe     1740\n",
       "B-per     1668\n",
       "I-org     1470\n",
       "I-geo      690\n",
       "I-tim      549\n",
       "B-art       75\n",
       "B-eve       53\n",
       "I-gpe       51\n",
       "I-eve       47\n",
       "I-art       43\n",
       "B-nat       30\n",
       "I-nat       11\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a higher sample of \"org\" tags in order for the model to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96652, 9665, 3346, 13011)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_neg = t2.loc[t2.tag1!=\"org\",:]\n",
    "t2_neg1 = t2_neg.sample(frac=0.1)\n",
    "\n",
    "t2_pos = t2.loc[t2.tag1==\"org\",:]\n",
    "t3 = pd.concat([t2_neg1,t2_pos],axis=0)\n",
    "t3 = t3.reset_index()\n",
    "len(t2_neg),len(t2_neg1),len(t2_pos),len(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Split for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = t3[\"tag1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(test_size=0.2,random_state=42,n_splits=1)\n",
    "\n",
    "for train_index, test_index in sss.split(t3, tgt):\n",
    "    x_train, x_test = t3[t3.index.isin(train_index)], t3[t3.index.isin(test_index)]\n",
    "    y_train, y_test = t3.loc[t3.index.isin(train_index),\"tag1\"], t3.loc[t3.index.isin(test_index),\"tag1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We would be also separating the string and numeric columns. The categorical columns will get an embedding layer and the numeric columns will get added to the flattened layers of the embedding output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['length','sent_len', 'rel_position']\n",
    "\n",
    "str_cols = ['word', 'pos_tag', 'shape', 'lemma', 'word_type',\n",
    "        'prev_word', 'word_next', 'prev_prev_word',\n",
    "       'word_next_next', 'prev_lemma', 'lemma_next', 'prev_prev_lemma',\n",
    "       'lemma_next_next', 'prev_shape', 'shape_next', 'prev_prev_shape',\n",
    "       'shape_next_next', 'prev_pos_tag', 'pos_tag_next', 'prev_prev_pos_tag',\n",
    "       'pos_tag_next_next','prev_word_type', 'word_type_next', 'prev_prev_word_type',\n",
    "       'word_type_next_next']\n",
    "\n",
    "x_train_num = x_train[num_cols]\n",
    "x_test_num = x_test[num_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tokenization and label encoding: Input to the embedding layer is a label encoded categorical variables (“texts_to_sequences”). Once encoded this also needs to be padded with the required length. Since we have only single tokens per column we are keeping the max_len to 1. The tokenization gets the mapping words to indexes from the train and this is applied to text. Any unknown text in the test set will be ignored\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def conv_str_cols(col_tr,col_te):\n",
    "    #print(col_tr)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(col_tr)\n",
    "    \n",
    "    col_tr1 = tokenizer.texts_to_sequences(col_tr)\n",
    "    col_te1 = tokenizer.texts_to_sequences(col_te)\n",
    "    col_tr2 = pad_sequences(col_tr1, maxlen=1, dtype='int32', padding='pre')\n",
    "    col_te2 = pad_sequences(col_te1, maxlen=1, dtype='int32', padding='pre')\n",
    "    \n",
    "    return col_tr2,col_te2,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n",
      "(10408, 1) (2603, 1)\n"
     ]
    }
   ],
   "source": [
    "tkn_list = []\n",
    "for num,i in enumerate(str_cols):\n",
    "    \n",
    "    var1,var2,tkn= conv_str_cols(x_train[i],x_test[i])\n",
    "    tkn_list.append(tkn)\n",
    "    print (var1.shape,var2.shape)\n",
    "    if(num==0):\n",
    "    \n",
    "        var_all_train = var1\n",
    "        var_all_test = var2\n",
    "       \n",
    "    else:\n",
    "        var_all_train = np.concatenate([var_all_train,var1],axis=1)\n",
    "        var_all_test = np.concatenate([var_all_test,var2],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code does the following\n",
    "\n",
    "1.\tCreates the embeddings for each of the categorical variable and flattens them\n",
    "2.\tAppends all of the independent inputs into a list (inputs)\n",
    "3.\tAppends the flattened outputs into another list called “dense” list object (outputs)\n",
    "4.\tCreates separate numeric input and appends it to the “dense” list object\n",
    "5.\tConnects the numeric input to “num_inp1” to the “dense” list object\n",
    "6.\tConcatenates all the embeddings and the numeric input to 1 layer\n",
    "7.\tdf_list is the list of all categorical and numeric train inputs\n",
    "8.\tdf_list_test is the list of all categorical and numeric test inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embed_size=0\n",
    "models = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "dense = []\n",
    "df_list = []\n",
    "df_list_test=[]\n",
    "for num,categoical_var in enumerate(range(var_all_train.shape[1]) ):\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    no_of_unique_cat =np.max(var_all_train[:,num]) + 1\n",
    "    \n",
    "    embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "    embedding_size = int(embedding_size)\n",
    "    embed_size = embed_size + embedding_size\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    \n",
    "    \n",
    "    A1 = Input(shape=(1,))\n",
    "    A2 = Embedding(vocab,embedding_size,input_length = 1)(A1)\n",
    "    \n",
    "    A3 = Flatten()(A2)\n",
    "    dense.append(A3)\n",
    "    inputs.append(A1)\n",
    "    \n",
    "       \n",
    "    df_list.append(var_all_train[:,num])\n",
    "    df_list_test.append(var_all_test[:,num])\n",
    "\n",
    "    \n",
    "num_shape = x_train_num.shape[1]\n",
    "embed_size = embed_size + num_shape\n",
    "num_inp = Input(shape=(num_shape,))\n",
    "num_inp1 = Dense(num_shape,activation = 'relu')(num_inp)\n",
    "dense.append(num_inp1)\n",
    "inputs.append(num_inp)\n",
    "\n",
    "st_size = int(embed_size/2)\n",
    "\n",
    "df_list.append(x_train_num)\n",
    "df_list_test.append(x_test_num)\n",
    "\n",
    "merge_one = concatenate(dense)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer merge_one will now be connected to other dense layers. I am going to reduce the number of nodes in each of the following layers by 2 and obtain a list of layer parameters. “layers_list” contains the architecture of the neural network after the merge_one. The function get_nn_mod lays out the rest of the architecture from merge_one based on the “layers_list”.  “Model” is a function which finally takes the initial inputs and the final outputs. The “model” object is now ready to be compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[153, 76, 38, 19, 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####flat layers list\n",
    "layers_list = []\n",
    "st_size1 = st_size\n",
    "while (st_size1>10):\n",
    "    st_size1 = int(st_size1/2)\n",
    "    layers_list.append(st_size1)\n",
    "\n",
    "layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_mod(list_layers,input1,dp,inputs):\n",
    "    layers = []\n",
    "\n",
    "    for num,i in enumerate(list_layers):\n",
    "        print(num,i)\n",
    "        \n",
    "        if(num==0):\n",
    "            input_orig = input1\n",
    " \n",
    "            \n",
    "        else:\n",
    "            input1 = Dense(i, activation='relu')(input1)\n",
    "            input1 = Dropout(dp)(input1)\n",
    "     \n",
    "        \n",
    "    \n",
    "    input_last = Dense(2, activation='softmax')(input1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=input_last)\n",
    "\n",
    "    opt = SGD(lr=0.01, clipnorm=1.)\n",
    "# Compile model\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 153\n",
      "1 76\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2 38\n",
      "3 19\n",
      "4 9\n"
     ]
    }
   ],
   "source": [
    "final_model = get_nn_mod(layers_list,merge_one,0.6,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        146700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 17)        578         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 2)         10          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 50)        140850      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 3)         18          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 50)        137700      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 50)        142250      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 50)        128500      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 50)        138850      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 50)        131750      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 50)        135100      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 50)        122700      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 50)        131100      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 3)         18          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 3)         18          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 3)         18          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 3)         18          input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1, 17)        595         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 1, 16)        512         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 1, 16)        528         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 1, 16)        528         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 1, 3)         21          input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 1, 3)         21          input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 1, 3)         21          input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 1, 3)         21          input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 17)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 50)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 3)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 50)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 50)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 50)           0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 50)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 50)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 50)           0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 50)           0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 50)           0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 3)            0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 3)            0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 3)            0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 3)            0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 17)           0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 16)           0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 16)           0           embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 16)           0           embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 3)            0           embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 3)            0           embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 3)            0           embedding_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 3)            0           embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            12          input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 614)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "                                                                 flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 76)           46740       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 76)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 38)           2926        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 38)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 19)           741         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 19)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 9)            180         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 9)            0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            20          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,409,044\n",
      "Trainable params: 1,409,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the dependant variable to categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train1 = le.fit_transform(y_train)\n",
    "y_test1 = le.fit_transform(y_test)\n",
    "\n",
    "y_train2 = to_categorical(y_train1)\n",
    "y_test2 = to_categorical(y_test1)\n",
    "y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could tweak this by adding class_weight example :class_weight = {0:8,1:6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      " - 13s - loss: 0.6911 - acc: 0.6959\n",
      "Epoch 2/30\n",
      " - 9s - loss: 0.5837 - acc: 0.7416\n",
      "Epoch 3/30\n",
      " - 8s - loss: 0.5762 - acc: 0.7427\n",
      "Epoch 4/30\n",
      " - 9s - loss: 0.5737 - acc: 0.7424\n",
      "Epoch 5/30\n",
      " - 9s - loss: 0.5799 - acc: 0.7428\n",
      "Epoch 6/30\n",
      " - 9s - loss: 0.5642 - acc: 0.7426\n",
      "Epoch 7/30\n",
      " - 9s - loss: 0.5402 - acc: 0.7427\n",
      "Epoch 8/30\n",
      " - 9s - loss: 0.4811 - acc: 0.7427\n",
      "Epoch 9/30\n",
      " - 8s - loss: 0.4500 - acc: 0.7422\n",
      "Epoch 10/30\n",
      " - 8s - loss: 0.4487 - acc: 0.7983\n",
      "Epoch 11/30\n",
      " - 8s - loss: 0.4208 - acc: 0.8413\n",
      "Epoch 12/30\n",
      " - 9s - loss: 0.4297 - acc: 0.8395\n",
      "Epoch 13/30\n",
      " - 9s - loss: 0.4372 - acc: 0.8466\n",
      "Epoch 14/30\n",
      " - 8s - loss: 0.4589 - acc: 0.8486\n",
      "Epoch 15/30\n",
      " - 9s - loss: 0.4370 - acc: 0.8554\n",
      "Epoch 16/30\n",
      " - 9s - loss: 0.4275 - acc: 0.8592\n",
      "Epoch 17/30\n",
      " - 9s - loss: 0.4250 - acc: 0.8641\n",
      "Epoch 18/30\n",
      " - 8s - loss: 0.4242 - acc: 0.8692\n",
      "Epoch 19/30\n",
      " - 8s - loss: 0.4421 - acc: 0.8705\n",
      "Epoch 20/30\n",
      " - 9s - loss: 0.4336 - acc: 0.8732\n",
      "Epoch 21/30\n",
      " - 9s - loss: 0.4341 - acc: 0.8743\n",
      "Epoch 22/30\n",
      " - 9s - loss: 0.4263 - acc: 0.8762\n",
      "Epoch 23/30\n",
      " - 9s - loss: 0.4065 - acc: 0.8816\n",
      "Epoch 24/30\n",
      " - 8s - loss: 0.4134 - acc: 0.8811\n",
      "Epoch 25/30\n",
      " - 9s - loss: 0.4225 - acc: 0.8829\n",
      "Epoch 26/30\n",
      " - 9s - loss: 0.4127 - acc: 0.8834\n",
      "Epoch 27/30\n",
      " - 8s - loss: 0.4030 - acc: 0.8826\n",
      "Epoch 28/30\n",
      " - 8s - loss: 0.3881 - acc: 0.8848\n",
      "Epoch 29/30\n",
      " - 9s - loss: 0.3909 - acc: 0.8880\n",
      "Epoch 30/30\n",
      " - 8s - loss: 0.3901 - acc: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x33017c50>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_model.fit(df_list,y_train2, batch_size=10, epochs=30,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_list_test),len(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the predicted outout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=final_model.predict(df_list_test)\n",
    "pred = pred1.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1840\n",
       "1     763\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1840\n",
       "1     763\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9093353822512485 0.8863312046848031\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "ac1 = accuracy_score(y_test1, pred)\n",
    "print (ac1, f1_score(y_test1, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_name = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1769</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>598</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1  act\n",
       "0  1769  165    0\n",
       "1    71  598    1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#pred_inv = le.inverse_transform(pred)\n",
    "\n",
    "cmat = pd.DataFrame(confusion_matrix(y_test1, pred, labels=[0,1], sample_weight=None))\n",
    "cmat.columns = rows_name \n",
    "cmat[\"act\"] = rows_name\n",
    "cmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and tokenizer for doing the model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pik_now(ob_name):\n",
    "    fl_out1 = fl_out + \"/\" + ob_name\n",
    "    pickling_on = open(fl_out1,\"wb\")\n",
    "    pickle.dump(eval(ob_name), pickling_on)\n",
    "    pickling_on.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_out =  \"model_punc\"\n",
    "pikl_list = [\"tkn_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pikl_list:\n",
    "    pik_now(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "final_model.save('model_punc/model_ner.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2-py36]",
   "language": "python",
   "name": "conda-env-Anaconda2-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
